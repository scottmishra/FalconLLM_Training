{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM4BlMvo9JxLXIZRTZe0/RX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scottmishra/FalconLLM_Training/blob/main/FalconLLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dj7dt9K0y5__"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers einops accelerate langchain bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "Me3D5pYUzcQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import HuggingFacePipeline\n",
        "from transformers import AutoTokenizer, pipeline\n",
        "\n",
        "import torch\n",
        "\n",
        "model = \"tiiuae/falcon-7b-instruct\" ## this will pull from hugging face, based on the model card\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "\n",
        "pipeline = pipeline(\n",
        "    \"text-generation\", ## Task of the pipeline\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\", #manages cpu and gpu memory, comes from accelerate\n",
        "    max_length=200, #output token limit\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "## running this cell will pull the model down on to the colab, and takes\n",
        "## about 15 GB for the 7b model along with the checkpoints\n",
        "## the colab download speed helps this take about 2 mins, but\n",
        "## it can take longer if you were to run this on your own machine"
      ],
      "metadata": {
        "id": "bzv6sL2DzhPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## model_kwargs allows use to adjust the setup of the model a bit\n",
        "## At this point the LLM is now in GPU memory\n",
        "llm = HuggingFacePipeline(pipeline=pipeline, model_kwargs = {'temperature':0})"
      ],
      "metadata": {
        "id": "JDc7lbpW0hEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## This takes just a few seconds and shows the simplest way to pull in the model\n",
        "llm(\"What is capital of India?\")"
      ],
      "metadata": {
        "id": "DSUaktw11OSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate, LLMChain\n",
        "\n",
        "template=\"\"\"\n",
        "You are an intelligent chatbot. Provide a truthful answer to the following question.\n",
        "Questions: {question}\n",
        "Answer:\"\"\"\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
        "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
        "question = \"Example why the Chicago Cubs are so beloved in a nursery rhyme\"\n",
        "\n",
        "print(llm_chain.run(question))"
      ],
      "metadata": {
        "id": "cVKIP_6K1XtY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Starting a QLoRA fine tuning approach"
      ],
      "metadata": {
        "id": "QI2akhX33FT8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HE9SnBrc3Iry"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}